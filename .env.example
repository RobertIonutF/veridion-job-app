# --- Scraper settings ---
# Input CSV of websites to crawl
SCRAPE_INPUT=data/sample-websites.csv
# Output JSON with raw scraped signals
SCRAPED_OUT=out/scraped.json
# Concurrency of parallel HTTP fetches
CONCURRENCY=40
# Limit number of rows to process (0 = no limit)
LIMIT=0
# Request timeout in ms
REQUEST_TIMEOUT_MS=20000
# Enable verbose scraper logs (set to '1')
DEBUG_SCRAPE=0
# Accept invalid TLS certificates (set to '1' only for local dev)
INSECURE_TLS=0
# Optional proxy (e.g. http://user:pass@host:port)
# HTTPS_PROXY=
# HTTP_PROXY=

# --- Merge step ---
# Input CSV mapping websites to company names
NAMES_INPUT=data/sample-websites-company-names.csv
# Output merged profiles JSON
PROFILES_OUT=out/profiles.json

# --- Index & API ---
# Where to write/read the search index JSON
INDEX_PATH=out/index.json
# Fastify server port
PORT=3000

# CORS allowlist (comma-separated origins). Example: https://example.com,https://studio.local
# If empty, CORS is disabled.
ORIGINS=

# In-memory IP rate limiting
# Max requests per window (defaults used by server if unset)
RATE_LIMIT_MAX=120
# Window size in ms
RATE_LIMIT_WINDOW_MS=60000

# Matching workload caps (protects latency on large indexes)
# Max candidate ids collected before scoring
MAX_CANDIDATES=2000
# Max profiles to brute-force when fuzzy search returns nothing
MAX_BRUTE_FORCE=5000

# UI sample inputs CSV (used by the demo pages)
API_INPUT_SAMPLE=data/API-input-sample.csv

# --- UI ---
# No specific UI env vars yet; templates live in src/views
